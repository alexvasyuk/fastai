{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSLRjI5tokCN6Yp9gmo2xo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexvasyuk/fastai_deep_learning_course/blob/main/Lesson_5%2C6_Binary_Split%2C_Decision_Trees%2C_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Split"
      ],
      "metadata": {
        "id": "kTn8NaTb8LMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and set screen proportions\n",
        "from fastai.imports import *\n",
        "np.set_printoptions(linewidth=130)"
      ],
      "metadata": {
        "id": "XY4xzRvi8g8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload kaggle.json to Google Collab. Get it inside the profile of your Kaggle account\n",
        "from google.colab import files\n",
        "files.upload()  # upload kaggle.json from your computer"
      ],
      "metadata": {
        "id": "39w5K-fYAZS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move kaggle.json to the right place in Google directory\n",
        "!cp kaggle.json /root/.config/kaggle/\n",
        "!chmod 600 /root/.config/kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "r9zjO0skAbea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Titanic data from Kaggle\n",
        "import os\n",
        "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
        "\n",
        "if iskaggle: path = Path('../input/titanic')\n",
        "else:\n",
        "    import zipfile,kaggle\n",
        "    path = Path('titanic')\n",
        "    kaggle.api.competition_download_cli(str(path))\n",
        "    zipfile.ZipFile(f'{path}.zip').extractall(path)\n",
        "\n",
        "df = pd.read_csv(path/'train.csv')\n",
        "tst_df = pd.read_csv(path/'test.csv')\n",
        "modes = df.mode().iloc[0]"
      ],
      "metadata": {
        "id": "iISNFH5N_hmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pre-process input data\n",
        "\n",
        "def proc_data(df):\n",
        "    df['Fare'] = df.Fare.fillna(0) # You want to fill missing/corrupted Fare 0, because it will be distinct when splitting occurs,\n",
        "                                  # and the algorithm will pick it up as a separate group. If you use mode for cont. variable, you'll\n",
        "                                  # create an arbitrary spike, which can confuse the algorithm.\n",
        "    df.fillna(modes, inplace=True) # Everything else replace missing values with mode\n",
        "    df['LogFare'] = np.log1p(df['Fare']) # Still need to log Fare because it is highly right skewed\n",
        "    df['Embarked'] = pd.Categorical(df.Embarked) # Marking for BinarySplit that Embarked is a category. This means its discrete and its values are limited.\n",
        "    df['Sex'] = pd.Categorical(df.Sex) # Same as \"Embarked\".\n",
        "\n",
        "proc_data(df)\n",
        "proc_data(tst_df)"
      ],
      "metadata": {
        "id": "KdGq8ZKUBu9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mark features based on the type of data they hold\n",
        "cats=[\"Sex\",\"Embarked\"] # categories\n",
        "conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"] # continuous. WHy PClass not categorical: class 1 < 2 < 3.\n",
        "                                                    # If you mark it as cat. the algorithm won't pick up that ordinal relationship\n",
        "dep=\"Survived\""
      ],
      "metadata": {
        "id": "RqLF1VK1Db8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look at some data\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a matplotlib figure with 1 row and 2 columns of subplots, sized 11x5 inches\n",
        "fig,axs = plt.subplots(1,2, figsize=(11,5))\n",
        "\n",
        "# Create a bar plot showing average survival rate (y) for each Sex category (x) in the dataset\n",
        "# Place it in the first subplot and set its title\n",
        "sns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\n",
        "\n",
        "# Create a count plot showing the number of passengers for each Sex category\n",
        "# Place it in the second subplot and set its title\n",
        "sns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");"
      ],
      "metadata": {
        "id": "LSjNEvXfG3gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "\n",
        "# Import train_test_split function for splitting data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set a random seed for reproducibility so the train/validation split is always the same\n",
        "random.seed(42)\n",
        "\n",
        "# Split the dataframe 'df' into training (75%) and validation (25%) sets\n",
        "trn_df, val_df = train_test_split(df, test_size=0.25)\n",
        "\n",
        "# Convert categorical columns in the training set into integer codes (e.g., male=0, female=1)\n",
        "trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\n",
        "\n",
        "# Convert categorical columns in the validation set into integer codes using the same method\n",
        "val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)"
      ],
      "metadata": {
        "id": "HpW1VPziIXsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to extract feature matrix (X) and target vector (y) from a dataframe\n",
        "def xs_y(df):\n",
        "    # Create a copy of the dataframe containing only the categorical and continuous feature columns\n",
        "    xs = df[cats + conts].copy()\n",
        "\n",
        "    # Return the features (xs) and the target column (dep) if it exists in df, otherwise return None for y\n",
        "    return xs, df[dep] if dep in df else None\n",
        "\n",
        "# Extract features and target for the training set\n",
        "trn_xs, trn_y = xs_y(trn_df)\n",
        "\n",
        "# Extract features and target for the validation set\n",
        "val_xs, val_y = xs_y(val_df)"
      ],
      "metadata": {
        "id": "TfdX-DYRIyJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The prediction is just checking whether the person is female\n",
        "preds = val_xs.Sex==0"
      ],
      "metadata": {
        "id": "ZnzVEpSbJf2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the error of our 'prediction' by Splitting on 'Sex'\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "metadata": {
        "id": "Jt4CJSZMKcXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "POIwUFhRTDQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's split on somethign else - Fare people paid"
      ],
      "metadata": {
        "id": "vhYp43LVLMQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a filtered DataFrame containing only rows where LogFare is greater than 0\n",
        "df_fare = trn_df[trn_df.LogFare > 0]\n",
        "\n",
        "# Create a matplotlib figure (fig) and an array of axes objects (axs) with 1 row and 2 columns of subplots, sized 11x5 inches\n",
        "# fig = the overall container for the plots\n",
        "# axs = a NumPy array of subplot \"axes\" where each plot will be drawn\n",
        "fig, axs = plt.subplots(1, 2, figsize=(11, 5))\n",
        "\n",
        "# Create a box plot showing the distribution of LogFare for each survival category, drawn in the first subplot\n",
        "sns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\n",
        "\n",
        "# Create a KDE (Kernel Density Estimate - histogram on steroid) plot showing the smoothed distribution of LogFare, drawn in the second subplot - I didn't understand what this meant\n",
        "sns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n",
        "\n",
        "# this basically boils down to that most people died on the titanic\n",
        "# KDE shows that most people paid fare around logfare2.2\n",
        "# Most people who survived paid fare around logfare3.3"
      ],
      "metadata": {
        "id": "zncOO3L_Ml48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting on fare\n",
        "preds = val_xs.LogFare>2.7"
      ],
      "metadata": {
        "id": "95PwSaCZSDE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's the accuracy. Looks worse than splitting on Sex\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "metadata": {
        "id": "wApHJOKzTEO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ❓ How do we split on continuous vars (like LogFare)?\n",
        "# 👉 Categorical vars (like Sex) split naturally, but continuous vars need a threshold t.\n",
        "#    We try many t’s programmatically instead of eyeballing graphs.\n",
        "\n",
        "# ❓ What’s this whole business w/ s.d. and weighting?\n",
        "# 👉 For continuous vars (like LogFare), we pick a threshold t and split rows into 2 groups:\n",
        "#    left = all rows where X <= t, right = all rows where X > t.\n",
        "#    Then we look at y (Survived) inside each group.\n",
        "# ✅ A perfect split is when one group is all y=0 and the other all y=1.\n",
        "#    Standard deviation of y measures how \"mixed\" each group is: low = pure, high = messy.\n",
        "#    We weight std by group size so that big messy groups count more than tiny pure ones.\n",
        "\n",
        "# ❓ Why use standard deviation of y after a split?\n",
        "# 👉 For binary y (0/1), std is low if the group is “pure” (all died or all survived)\n",
        "#    and highest when it’s a 50/50 mix. So std is a measure of impurity.\n",
        "\n",
        "# ❓ Why multiply std by group size (weighting)?\n",
        "# 👉 Because a split that makes a tiny group perfectly pure shouldn’t fool us into\n",
        "#    thinking it’s a great split if the large group is still messy. Weighting reflects\n",
        "#    the impact of each group on the whole dataset.\n",
        "\n",
        "# ❓ Why average the stds from left & right groups?\n",
        "# 👉 To combine them into one overall impurity score for this candidate split. The tree\n",
        "#    needs a single number to compare “split at 2.7” vs “split at 3.5” etc.\n",
        "\n",
        "# ✅ Final intuition: A good split is one where each side of the threshold produces\n",
        "#    groups of passengers whose survival outcomes are as consistent (low std) as possible,\n",
        "#    especially for the larger groups."
      ],
      "metadata": {
        "id": "r_PeUNqAkqD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to score one side of a binary split\n",
        "# 'side' is a boolean mask selecting the rows, 'y' is the dependent variable (target)\n",
        "# Example: side = val_xs['LogFare'] > 2.7\n",
        "# 0    False\n",
        "# 1     True\n",
        "# 2    False\n",
        "# 3     True\n",
        "# 4    False\n",
        "# Name: LogFare, dtype: bool\n",
        "def _side_score(side, y):\n",
        "\n",
        "    # Count how many rows are in this side (number of True values in the mask)\n",
        "    tot = side.sum()\n",
        "\n",
        "    # If the side has 0 or 1 rows, it can't be split meaningfully, so return 0\n",
        "    if tot <= 1: return 0\n",
        "\n",
        "    # Otherwise, calculate the standard deviation of y in this side\n",
        "    # Multiply by the group size so bigger groups have more influence on the score\n",
        "    return y[side].std() * tot"
      ],
      "metadata": {
        "id": "xwn_pejvTF25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate the impurity score of a split on one column\n",
        "def score(col, y, split):\n",
        "\n",
        "    # Create a boolean mask for the left-hand side (rows where col <= split)\n",
        "    lhs = col <= split\n",
        "\n",
        "    # Compute the total impurity score as:\n",
        "    #   score of the left side + score of the right side\n",
        "    # Divide by total number of rows to normalize\n",
        "    return (_side_score(lhs, y) + _side_score(~lhs, y)) / len(y)\n"
      ],
      "metadata": {
        "id": "Ftk6sDxwFt0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score(trn_xs[\"Sex\"], trn_y, 0.5)"
      ],
      "metadata": {
        "id": "uxnI38bThkbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score(trn_xs[\"LogFare\"], trn_y, 2.7)"
      ],
      "metadata": {
        "id": "QMGjQ7SaiQUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that calculates the impurity score for a chosen column and split value\n",
        "def iscore(nm, split):\n",
        "    # Select the column 'nm' from the training feature matrix\n",
        "    col = trn_xs[nm]\n",
        "\n",
        "    # Return the impurity score for this column at the given split point\n",
        "    return score(col, trn_y, split)\n",
        "\n",
        "\n",
        "# Import interact widget from ipywidgets (lets you create sliders/dropdowns in Jupyter)\n",
        "from ipywidgets import interact\n",
        "\n",
        "# Create an interactive widget:\n",
        "# - 'nm' dropdown lets you pick from the continuous variables (conts)\n",
        "# - 'split' slider lets you move the split value (default set to 15.5)\n",
        "# Whenever you change them, 'iscore' is called and the score is printed\n",
        "interact(nm=conts, split=15.5)(iscore);\n"
      ],
      "metadata": {
        "id": "r-YNfF0NibWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to automatically find the best split value for column \"Age\"\n",
        "nm = \"Age\"\n",
        "col = trn_xs[nm]\n",
        "unq = col.unique()\n",
        "unq.sort()\n",
        "scores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\n",
        "unq[scores.argmin()] # returns the index of the smallest value in an array"
      ],
      "metadata": {
        "id": "VIm5_yjVtJeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here's the same thing in function form\n",
        "def min_col(df, nm):\n",
        "    col,y = df[nm],df[dep]\n",
        "    unq = col.dropna().unique()\n",
        "    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n",
        "    idx = scores.argmin()\n",
        "    return unq[idx],scores[idx]\n",
        "\n",
        "min_col(trn_df, \"Age\")"
      ],
      "metadata": {
        "id": "oic8gUqdG4Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do the same for every column\n",
        "cols = cats+conts\n",
        "{o:min_col(trn_df, o) for o in cols}"
      ],
      "metadata": {
        "id": "dAhBF8dIHGJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is called a R1 classifier"
      ],
      "metadata": {
        "id": "64yF8ZxfHRUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "# The idea is simple - once you've found the best split on the data (in oru case its Sex)\n",
        "# find the next best split for each of the values - Female and Male.\n",
        "\n",
        "#Get only female rows; get only male rows\n",
        "cols.remove(\"Sex\")\n",
        "ismale = trn_df.Sex==1\n",
        "males,females = trn_df[ismale],trn_df[~ismale]"
      ],
      "metadata": {
        "id": "G0IlIdavHdrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best split for males\n",
        "{o:min_col(males, o) for o in cols}"
      ],
      "metadata": {
        "id": "sXyhjMolK8lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best split for females\n",
        "{o:min_col(females, o) for o in cols}"
      ],
      "metadata": {
        "id": "nAEgOlrKK_5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We've just created a tree. 1st check Sex. If female, check Pclass <=2\n",
        "# If male, check Age <= 6. But this is all done manually. We need an automatic way to do it.\n",
        "# Enter DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "JUWJ8duFMRpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "\n",
        "m = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);"
      ],
      "metadata": {
        "id": "sRAJVD8eMtbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To draw the result\n",
        "import graphviz\n",
        "\n",
        "def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n",
        "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n",
        "                      special_characters=True, rotate=False, precision=precision, **kwargs)\n",
        "    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))"
      ],
      "metadata": {
        "id": "vXf6tXJkM1PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_tree(m, trn_xs)"
      ],
      "metadata": {
        "id": "GFdMtOOGM3Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another way to caluclate impurity is using gini\n",
        "def gini(cond):\n",
        "    act = df.loc[cond, dep]\n",
        "    return 1 - act.mean()**2 - (1-act).mean()**2"
      ],
      "metadata": {
        "id": "SxYEXAxUYG02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How accurate is this decision tree?\n",
        "mean_absolute_error(val_y, m.predict(val_xs))\n",
        "# turns out a bit worse than what we predicted if we just split on Sex, which was .215"
      ],
      "metadata": {
        "id": "QyNX305RcOgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets try a bigger decision tree\n",
        "m = DecisionTreeClassifier(min_samples_leaf=50)\n",
        "m.fit(trn_xs, trn_y)\n",
        "draw_tree(m, trn_xs, size=12)"
      ],
      "metadata": {
        "id": "lqpowqZWeHsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#looks like an improvement\n",
        "mean_absolute_error(val_y, m.predict(val_xs))"
      ],
      "metadata": {
        "id": "mTGtYH2KeRPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random forests - a bunch of randomly generated trees averaged out\n",
        "# this is a fix to just doing one deep tree, because with little data at the bottom\n",
        "# its predictions arent very good\n",
        "# but if you make a lot of such deep 'noisy' trees and average them out - you get good preds."
      ],
      "metadata": {
        "id": "tLuV1iKbiIAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tree(prop=0.75):\n",
        "    # Define a function that trains a decision tree on a random subset of the data.\n",
        "    # 'prop' is the proportion of the dataset to use (default = 75%).\n",
        "\n",
        "    n = len(trn_y)\n",
        "    # Get the total number of training samples.\n",
        "\n",
        "    idxs = random.choice(n, int(n*prop))\n",
        "    # Randomly choose 'prop' fraction of the dataset indices (with replacement).\n",
        "    # This creates the bootstrap sample used to train one tree.\n",
        "    # e.g. random.choice(10, int(10*0.75)) => array([5, 1, 7, 7, 0, 2, 9])\n",
        "\n",
        "    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n",
        "    # Create a Decision Tree classifier with a minimum of 5 samples per leaf.\n",
        "    # Fit (train) this tree on the randomly chosen subset of training data.\n",
        "    # Return the trained tree.\n"
      ],
      "metadata": {
        "id": "fDZs-MLMj0xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets try it manually\n",
        "trees = [get_tree() for t in range(100)]\n",
        "all_probs = [t.predict(val_xs) for t in trees]\n",
        "avg_probs = np.stack(all_probs).mean(0)\n",
        "mean_absolute_error(val_y, avg_probs)"
      ],
      "metadata": {
        "id": "TPAFsQ7_kCVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In a Random Forest, there are TWO types of randomness that make the trees diverse:\n",
        "#\n",
        "# 1. Random rows (data samples) = \"bagging\"\n",
        "#    - For each tree, we pick a random subset of the rows (observations), with replacement.\n",
        "#     \"With replacement\" means when we randomly sample rows for each tree,\n",
        "#     we allow the same row to be picked more than once.\n",
        "#     So some rows may be duplicated in the sample, while others are left out.\n",
        "#     This bootstrap sampling makes each tree see a slightly different version\n",
        "#     of the dataset, reducing correlation between trees.\n",
        "#    - Example: if we have 10,000 rows, one tree may only see ~7,500 randomly chosen rows.\n",
        "#    - This reduces variance since each tree trains on different data.\n",
        "#\n",
        "# 2. Random columns (features)\n",
        "#    - At each split in the tree, the algorithm looks at only a random subset of the features.\n",
        "#    - Example: if there are 10 features, a given split might only consider 3 of them.\n",
        "#    - This prevents the same \"strong\" feature (e.g. Sex=female in Titanic) from dominating every tree.\n",
        "#    - This increases diversity, since trees use different \"perspectives\" of the data.\n",
        "#\n",
        "# 🔑 Why both?\n",
        "# - Random rows = each tree sees different examples\n",
        "# - Random columns = each tree uses different subsets of features\n",
        "# - Together, this makes trees less correlated → averaging their predictions cancels out errors.\n"
      ],
      "metadata": {
        "id": "9rqRS80OlKTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(100, min_samples_leaf=5)\n",
        "# Create a random forest with 100 decision trees,\n",
        "# each tree must have at least 5 samples in every leaf node\n",
        "rf.fit(trn_xs, trn_y);\n",
        "mean_absolute_error(val_y, rf.predict(val_xs))"
      ],
      "metadata": {
        "id": "WZDtKq8VmFkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows which columns have the most effect on the outcome.\n",
        "pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');"
      ],
      "metadata": {
        "id": "uYwIpkCOmaUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}